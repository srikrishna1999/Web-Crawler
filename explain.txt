The web crawler begins its operation by selecting a subset of URLs (20 seeds) from the given seed list. For each URL, a Webpage object is created, encapsulating important attributes such as priority, domain, URL scheme, content type, page size, and page status.

To implement the crawling strategy, the crawler follows a breadth-first search methodology, parsing URLs level-wise based on their depth. Within each depth level, URLs are prioritized to ensure that new domains are processed before revisiting previously visited domains. This prioritization mechanism allows for the efficient discovery of new content. A priority queue is utilized to manage and sort the URLs based on depth and priority, enabling the crawler to process them in an orderly fashion.

As the crawler enters a loop that continues until all URLs in the priority queue are processed, it pops a URL from the queue for examination. The URL is validated to check its correctness and ensure that its Content-Type is "text/html." Additionally, the crawler verifies whether the URL belongs to the New Zealand domain. If the URL is valid, its content is fetched using urllib, and the crawler checks the domainâ€™s robots.txt file (if present) to determine if the URL can be crawled. Relevant details, such as status code, page size, and domain, are then stored for further analysis.

To eliminate duplicates and manage URLs effectively, the crawler maintains two separate sets: one for URLs that have already been crawled (url_seen) and another for domains that have been visited (domain_seen). This approach ensures that only unvisited domains are processed first, thus optimizing the crawling process.

After each URL is crawled, the details are logged in a web crawler log file, serving as a comprehensive record of the crawling activity, including the status and outcomes of each processed URL. Overall, this web crawler is designed to use a breadth-first search strategy, along with priority management and duplicate handling, ensuring efficient and systematic crawling of webpages, particularly focusing on the New Zealand domain.